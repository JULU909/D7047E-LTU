{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "694e28da-0df9-4464-8281-03f722044195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harish Vasanth\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:326: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  np.bool8: (False, True),\n",
      "c:\\Users\\Harish Vasanth\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:205: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  np.bool8: (False, True),\n",
      "c:\\Users\\Harish Vasanth\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from hyperparams import Hyperparameters as params\n",
    "from stable_baselines3.common.atari_wrappers import (\n",
    "    ClipRewardEnv, EpisodicLifeEnv, FireResetEnv, MaxAndSkipEnv, NoopResetEnv\n",
    ")\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c72f4b25-ab84-433a-866b-9ed3160a93a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        env = NoopResetEnv(env, noop_max=30)\n",
    "        env = MaxAndSkipEnv(env, skip=4)\n",
    "        env = EpisodicLifeEnv(env)\n",
    "        if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
    "            env = FireResetEnv(env)\n",
    "        env = ClipRewardEnv(env)\n",
    "        env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
    "        env = gym.wrappers.GrayScaleObservation(env)\n",
    "        env = gym.wrappers.FrameStack(env, 4)\n",
    "        env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "    return thunk\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, action_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x.float() / 255.0)  # Normalize pixel values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9f77fd3-0187-4ec4-b764-92d3e41c67a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harish Vasanth\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:219: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 5.65GB > 3.58GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\HARISH~1\\AppData\\Local\\Temp/ipykernel_10244/2630899278.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mreal_next_obs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# Assuming terminal observation is handled differently\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mrb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_next_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "run_name = f\"{params.env_id}__{params.exp_name}__{params.seed}__{int(time.time())}\"\n",
    "\n",
    "random.seed(params.seed)\n",
    "np.random.seed(params.seed)\n",
    "torch.manual_seed(params.seed)\n",
    "torch.backends.cudnn.deterministic = params.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Environment setup\n",
    "envs = gym.vector.SyncVectorEnv([make_env(params.env_id, params.seed, 0, params.capture_video, run_name)])\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
    "\n",
    "q_network = QNetwork(envs.single_action_space.n).to(device)\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=params.learning_rate)\n",
    "target_network = QNetwork(envs.single_action_space.n).to(device)\n",
    "target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "# Replay buffer setup\n",
    "rb = ReplayBuffer(\n",
    "    100_000,\n",
    "    envs.single_observation_space,\n",
    "    envs.single_action_space,\n",
    "    device,\n",
    "    optimize_memory_usage=False,\n",
    "    handle_timeout_termination=True,\n",
    ")\n",
    "\n",
    "def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
    "    slope = (end_e - start_e) / duration\n",
    "    return max(slope * t + start_e, end_e)\n",
    "\n",
    "obs = envs.reset()\n",
    "for global_step in range(params.total_timesteps):\n",
    "    epsilon = linear_schedule(params.start_e, params.end_e, params.exploration_fraction * params.total_timesteps, global_step)\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "        actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.as_tensor(obs, device=device, dtype=torch.float32)\n",
    "            q_values = q_network(state_tensor)\n",
    "        actions = torch.argmax(q_values, dim=1).cpu().numpy()\n",
    "\n",
    "    next_obs, rewards, dones, infos = envs.step(actions)\n",
    "\n",
    "    for idx, info in enumerate(infos):\n",
    "        if \"episode\" in info.keys():\n",
    "            print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "            break\n",
    "\n",
    "    real_next_obs = next_obs.copy()\n",
    "    for idx, done in enumerate(dones):\n",
    "        if done:\n",
    "            real_next_obs[idx] = None  # Assuming terminal observation is handled differently\n",
    "\n",
    "    rb.add(obs, real_next_obs, actions, rewards, dones, infos)\n",
    "\n",
    "    obs = next_obs\n",
    "\n",
    "    # Training\n",
    "    if global_step > params.learning_starts and global_step % params.train_frequency == 0:\n",
    "        data = rb.sample(params.batch_size)\n",
    "        state_batch = torch.tensor(data.observations, dtype=torch.float32, device=device)\n",
    "        next_state_batch = torch.tensor(data.next_observations, dtype=torch.float32, device=device)\n",
    "        action_batch = torch.tensor(data.actions, dtype=torch.long, device=device)\n",
    "        reward_batch = torch.tensor(data.rewards, dtype=torch.float32, device=device)\n",
    "        done_batch = torch.tensor(data.dones, dtype=torch.float32, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_values = target_network(next_state_batch).max(1)[0]\n",
    "            td_target = reward_batch + params.gamma * next_state_values * (1 - done_batch)\n",
    "\n",
    "        current_q_values = q_network(state_batch).gather(1, action_batch.unsqueeze(1)).squeeze()\n",
    "        loss = F.mse_loss(current_q_values, td_target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if global_step % params.target_network_frequency == 0:\n",
    "        target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "if params.save_model:\n",
    "    model_path = f\"runs/{run_name}/{params.exp_name}_model\"\n",
    "    torch.save(q_network.state_dict(), model_path)\n",
    "    print(f\"model saved to {model_path}\")\n",
    "\n",
    "envs.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c01a47-e532-4c06-a577-f5b351729f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
